<h1>ğŸ“„ Copiloto Conversacional sobre PDFs</h1>
<p>AplicaciÃ³n de <strong>Streamlit</strong> que procesa PDFs y permite hacer preguntas sobre su contenido usando <strong>IA local con Ollama</strong> y <strong>RAG</strong> (Retrieval-Augmented Generation), ejecutada Ã­ntegramente con <strong>Docker</strong> y <strong>Docker Compose</strong>.</p>

<h2>ğŸš€ CaracterÃ­sticas</h2>
<ul>
  <li><strong>Procesamiento de PDFs</strong>: extracciÃ³n de texto y chunking</li>
  <li><strong>IA local</strong>: modelo <code>llama3:8b</code> servido por Ollama</li>
  <li><strong>RAG</strong>: bÃºsqueda semÃ¡ntica con Chroma</li>
  <li><strong>Interfaz Web</strong>: app Streamlit en el puerto 8501</li>
  <li><strong>ContenerizaciÃ³n</strong>: entorno reproducible con Docker</li>
</ul>

<h2>ğŸ“‹ Requisitos</h2>
<ul>
  <li><strong>Docker</strong> (Docker Desktop en Windows/Mac o Docker Engine en Linux)</li>
  <li><strong>Docker Compose</strong></li>
</ul>

<h2>ğŸ› ï¸ InstalaciÃ³n y EjecuciÃ³n (solo Docker)</h2>

<h3>1) Clonar el repositorio</h3>
<pre><code>git clone &lt;tu-repositorio&gt;
cd copiloto_pdfs
</code></pre>

<h3>2) Construir las imÃ¡genes</h3>
<pre><code>docker compose build
</code></pre>
<p><em>Nota:</em> La primera vez puede demorar porque se instalan todas las dependencias dentro de la imagen. Las siguientes veces serÃ¡ mucho mÃ¡s rÃ¡pido gracias a la cachÃ© de Docker.</p>

<h3>3) Levantar servicios</h3>
<pre><code>docker compose up -d
</code></pre>
<p>Esto levanta dos contenedores:</p>
<ul>
  <li><code>ollama</code>: servidor del modelo (puerto 11434 interno)</li>
  <li><code>app</code>: aplicaciÃ³n Streamlit (expuesta en <code>http://localhost:8501</code>)</li>
</ul>

<h3>4) Descargar el modelo (solo primera vez)</h3>
<pre><code>docker compose exec ollama ollama pull llama3:8b
</code></pre>
<p><em>Importante:</em> La descarga del modelo puede tardar varios minutos la primera vez. Una vez descargado, quedarÃ¡ cacheado y los arranques posteriores serÃ¡n mucho mÃ¡s rÃ¡pidos.</p>

<h3>5) Abrir la aplicaciÃ³n</h3>
<pre><code>http://localhost:8501
</code></pre>

<h3>âœ”ï¸ Comando Ãºnico (levantar y abrir navegador)</h3>
<p><em>Linux:</em></p>
<pre><code>docker compose up -d &amp;&amp; xdg-open http://localhost:8501
</code></pre>
<p><em>macOS:</em></p>
<pre><code>docker compose up -d &amp;&amp; open http://localhost:8501
</code></pre>
<p><em>Windows (PowerShell):</em></p>
<pre><code>docker compose up -d; Start-Process "http://localhost:8501"
</code></pre>

<h2>ğŸ›‘ Detener</h2>
<pre><code>docker compose down
</code></pre>
<p>Esto apaga y elimina los contenedores (no borra las imÃ¡genes). Si alguna vez quisieras limpiar volÃºmenes y datos cacheados, usa <code>docker compose down -v</code>.</p>

<h2>ğŸ§ª Pruebas rÃ¡pidas</h2>
<ul>
  <li>Ver contenedores activos:
    <pre><code>docker compose ps
</code></pre>
  </li>
  <li>Probar el modelo desde el contenedor de Ollama:
    <pre><code>docker compose exec ollama ollama run llama3:8b "Hola, Â¿cÃ³mo estÃ¡s?"
</code></pre>
  </li>
</ul>

<h2>ğŸªµ Logs</h2>
<pre><code>docker compose logs -f app
docker compose logs -f ollama
</code></pre>

<h2>ğŸ“ Estructura del proyecto</h2>
<pre><code>copiloto_pdfs/
â”œâ”€â”€ app.py                 # AplicaciÃ³n principal de Streamlit
â”œâ”€â”€ requirements.txt       # Dependencias de Python (para la imagen)
â”œâ”€â”€ Dockerfile             # Imagen de la app
â”œâ”€â”€ docker-compose.yml     # OrquestaciÃ³n (app + ollama)
â””â”€â”€ README.md              # Este archivo
</code></pre>

<h2>â“Preguntas frecuentes</h2>
<ul>
  <li><strong>Â¿Por quÃ© tarda tanto la primera vez?</strong><br>
    Porque Docker construye la imagen (instala dependencias) y Ollama descarga el modelo <code>llama3:8b</code>. DespuÃ©s de esa primera preparaciÃ³n, los arranques siguientes usan la cachÃ© y son mucho mÃ¡s rÃ¡pidos.</li>
  <li><strong>Â¿Necesito Python/venv en mi mÃ¡quina?</strong><br>
    No. Todo corre dentro de los contenedores.</li>
  <li><strong>Â¿CÃ³mo actualizo el cÃ³digo?</strong><br>
    Cambia tus archivos y luego:
    <pre><code>docker compose build app
docker compose up -d
</code></pre>
  </li>
</ul>

<h2>ğŸ“š TecnologÃ­as</h2>
<ul>
  <li><strong>Streamlit</strong> â€” interfaz web</li>
  <li><strong>LangChain</strong> â€” orquestaciÃ³n de LLM + RAG</li>
  <li><strong>ChromaDB</strong> â€” vector store</li>
  <li><strong>Sentence Transformers</strong> â€” embeddings</li>
  <li><strong>Ollama</strong> â€” LLM local (modelo <code>llama3:8b</code>)</li>
  <li><strong>Docker &amp; Docker Compose</strong> â€” despliegue y orquestaciÃ³n</li>
</ul>

<h3>ğŸ¯ JustificaciÃ³n de la Stack TecnolÃ³gica</h3>
<ul>
  <li><strong>Streamlit</strong> â€” Front-end ideal para prototipos de IA, desarrollo "rÃ¡pido" y recomendado para proyectos con IA.</li>
  <li><strong>LangChain</strong> â€” Framework estÃ¡ndar para RAG con integraciÃ³n con Ollama</li>
  <li><strong>ChromaDB</strong> â€” Vector store ligero y eficiente para aplicaciones locales</li>
  <li><strong>Sentence Transformers</strong> â€” Embeddings de alta calidad en espaÃ±ol/inglÃ©s, optimizados para RAG</li>
  <li><strong>Ollama</strong> â€” LLM local sin dependencias externas, escogÃ­ el modelo llama3:8b por el balance calidad/velocidad</li>
  <li><strong>Docker</strong> â€” Para reproducir el entorno en otras mÃ¡quinas.</li>
</ul>
