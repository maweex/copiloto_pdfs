# ğŸ“„ Copiloto Conversacional sobre PDFs

Una aplicaciÃ³n de Streamlit que permite procesar PDFs y hacer preguntas sobre su contenido usando IA local (Ollama) y RAG (Retrieval-Augmented Generation).

## ğŸš€ CaracterÃ­sticas

- **Procesamiento de PDFs**: Extrae texto y crea chunks para anÃ¡lisis
- **IA Local**: Usa Ollama con el modelo llama3 para procesamiento local
- **RAG**: Sistema de bÃºsqueda semÃ¡ntica para respuestas contextuales
- **Interfaz Web**: AplicaciÃ³n Streamlit fÃ¡cil de usar
- **Persistencia**: Almacena embeddings en Chroma DB local

## ğŸ“‹ Requisitos Previos

- Python 3.8+
- Ollama instalado y configurado
- Modelo llama3 descargado

## ğŸ› ï¸ InstalaciÃ³n

### 1. Clonar el repositorio

```bash
git clone <tu-repositorio>
cd copiloto_pdfs
```

### 2. Crear y activar entorno virtual

#### Windows (PowerShell):

```powershell
# Crear entorno virtual
python -m venv venv

# Activar entorno virtual
.\venv\Scripts\Activate.ps1

# O usar el script automÃ¡tico
.\activate_venv.ps1
```

#### Windows (Command Prompt):

```cmd
# Crear entorno virtual
python -m venv venv

# Activar entorno virtual
venv\Scripts\activate.bat

# O usar el script automÃ¡tico
activate_venv.bat
```

#### Linux/Mac:

```bash
# Crear entorno virtual
python3 -m venv venv

# Activar entorno virtual
source venv/bin/activate
```

### 3. Instalar dependencias

```bash
pip install -r requirements.txt
```

### 4. Configurar Ollama

```bash
# Iniciar servicio Ollama
ollama serve

# Descargar modelo llama3
ollama pull llama3
```

## ğŸš€ Uso

### 1. Activar entorno virtual

```bash
# PowerShell
.\venv\Scripts\Activate.ps1

# Command Prompt
venv\Scripts\activate.bat

# Linux/Mac
source venv/bin/activate
```

### 2. Ejecutar la aplicaciÃ³n

```bash
streamlit run app.py
```

### 3. Usar la aplicaciÃ³n

1. Abre tu navegador en `http://localhost:8501`
2. Sube uno o mÃ¡s archivos PDF
3. Espera a que se procesen y generen resÃºmenes
4. Haz preguntas sobre el contenido de los PDFs

## ğŸ“ Estructura del Proyecto

```
copiloto_pdfs/
â”œâ”€â”€ app.py                 # AplicaciÃ³n principal de Streamlit
â”œâ”€â”€ requirements.txt       # Dependencias de Python
â”œâ”€â”€ activate_venv.bat     # Script de activaciÃ³n para Windows CMD
â”œâ”€â”€ activate_venv.ps1     # Script de activaciÃ³n para PowerShell
â”œâ”€â”€ venv/                 # Entorno virtual (se crea automÃ¡ticamente)
â”œâ”€â”€ data/                 # Datos persistentes (se crea automÃ¡ticamente)
â”‚   â””â”€â”€ chroma/          # Base de datos vectorial
â””â”€â”€ README.md            # Este archivo
```

## ğŸ”§ SoluciÃ³n de Problemas

### Error de conexiÃ³n a Ollama

Si ves el error "HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded":

1. **Verifica que Ollama estÃ© ejecutÃ¡ndose:**

   ```bash
   ollama serve
   ```

2. **Verifica que el modelo estÃ© disponible:**

   ```bash
   ollama list
   ```

3. **Si no hay modelos, descarga uno:**
   ```bash
   ollama pull llama3
   ```

### Problemas con el entorno virtual

1. **AsegÃºrate de activar el entorno virtual antes de instalar dependencias:**

   ```bash
   .\venv\Scripts\Activate.ps1  # PowerShell
   # o
   venv\Scripts\activate.bat    # CMD
   ```

2. **Verifica que estÃ©s en el entorno virtual:**

   - DeberÃ­as ver `(venv)` al inicio de tu prompt

3. **Si hay problemas, recrea el entorno:**
   ```bash
   deactivate
   rmdir /s venv
   python -m venv venv
   .\venv\Scripts\Activate.ps1
   pip install -r requirements.txt
   ```

## ğŸ“š Dependencias Principales

- **Streamlit**: Interfaz web
- **PyPDF2**: Procesamiento de PDFs
- **LangChain**: Framework para aplicaciones de IA
- **ChromaDB**: Base de datos vectorial
- **Sentence Transformers**: Embeddings de texto
- **Ollama**: LLM local

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

## ğŸ†˜ Soporte

Si tienes problemas:

1. Verifica que Ollama estÃ© ejecutÃ¡ndose
2. AsegÃºrate de estar en el entorno virtual
3. Revisa que todas las dependencias estÃ©n instaladas
4. Consulta los logs de error en la consola

---

**Nota**: Siempre activa el entorno virtual antes de trabajar en el proyecto:

```bash
.\venv\Scripts\Activate.ps1  # PowerShell
# o
venv\Scripts\activate.bat    # CMD
```
