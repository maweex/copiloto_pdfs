<h1>ğŸ“„ Copiloto Conversacional sobre PDFs</h1>
<p>AplicaciÃ³n de <strong>Streamlit</strong> que procesa PDFs y permite hacer preguntas sobre su contenido usando <strong>IA local con Ollama</strong> y <strong>RAG</strong> (Retrieval-Augmented Generation), ejecutada Ã­ntegramente con <strong>Docker</strong> y <strong>Docker Compose</strong>.</p>

<h2>ğŸš€ CaracterÃ­sticas</h2>
<ul>
  <li><strong>Procesamiento de PDFs</strong>: extracciÃ³n de texto y chunking</li>
  <li><strong>IA local</strong>: modelo <code>llama3:3b</code> servido por Ollama</li>
  <li><strong>RAG</strong>: bÃºsqueda semÃ¡ntica con Chroma</li>
  <li><strong>Interfaz Web</strong>: app Streamlit en el puerto 8501</li>
  <li><strong>ContenerizaciÃ³n</strong>: entorno reproducible con Docker</li>
</ul>

<h2>ğŸ“‹ Requisitos</h2>
<ul>
  <li><strong>Docker</strong> (Docker Desktop en Windows/Mac o Docker Engine en Linux)</li>
  <li><strong>Docker Compose</strong></li>
</ul>

<h2>ğŸ› ï¸ InstalaciÃ³n y EjecuciÃ³n (solo Docker)</h2>

<h3>1) Clonar el repositorio</h3>
<pre><code>git clone &lt;Link del repo&gt;
cd copiloto_pdfs
</code></pre>

<h3>2) Construir las imÃ¡genes</h3>
<pre><code>docker compose build
</code></pre>
<p><em>Nota:</em> La primera vez puede demorar porque se instalan todas las dependencias dentro de la imagen. Las siguientes veces serÃ¡ mucho mÃ¡s rÃ¡pido gracias a la cachÃ© de Docker. Recomiendo tener la App de Docker abierta.</p>

<h3>3) Levantar servicios</h3>
<pre><code>docker compose up -d
</code></pre>
<p>Esto levanta dos contenedores:</p>
<ul>
  <li><code>ollama</code>: servidor del modelo (puerto 11434 interno)</li>
  <li><code>app</code>: aplicaciÃ³n Streamlit (expuesta en <code>http://localhost:8501</code>)</li>
</ul>

<h3>4) Descargar el modelo (solo primera vez)</h3>
<pre><code>docker compose exec ollama ollama pull llama3:3b
</code></pre>
<p><em>Importante:</em> La descarga del modelo puede tardar varios minutos la primera vez. Una vez descargado, quedarÃ¡ cacheado y los arranques posteriores serÃ¡n mucho mÃ¡s rÃ¡pidos.</p>

<h3>5) Abrir la aplicaciÃ³n</h3>
<pre><code>http://localhost:8501
</code></pre>

<h3>âœ”ï¸ Comando Ãºnico (levantar y abrir navegador)</h3>
<p><em>Linux:</em></p>
<pre><code>docker compose up -d &amp;&amp; xdg-open http://localhost:8501
</code></pre>
<p><em>macOS:</em></p>
<pre><code>docker compose up -d &amp;&amp; open http://localhost:8501
</code></pre>
<p><em>Windows (PowerShell):</em></p>
<pre><code>docker compose up -d; Start-Process "http://localhost:8501"
</code></pre>

<h2>ğŸ›‘ Detener</h2>
<pre><code>docker compose down
</code></pre>
<p>Esto apaga y elimina los contenedores (no borra las imÃ¡genes). Si alguna vez quisieras limpiar volÃºmenes y datos cacheados, usa <code>docker compose down -v</code>.</p>

<h2>ğŸ§ª Pruebas rÃ¡pidas</h2>
<ul>
  <li>Ver contenedores activos:
    <pre><code>docker compose ps
</code></pre>
  </li>
  <li>Probar el modelo desde el contenedor de Ollama:
    <pre><code>docker compose exec ollama ollama run llama3:3b "Hola, Â¿cÃ³mo estÃ¡s?"
</code></pre>
  </li>
</ul>

<h2>ğŸªµ Logs</h2>
<pre><code>docker compose logs -f app
docker compose logs -f ollama
</code></pre>

<h2>ğŸ“ Estructura del proyecto</h2>
<pre><code>copiloto_pdfs/
â”œâ”€â”€ app.py                 # AplicaciÃ³n principal de Streamlit
â”œâ”€â”€ requirements.txt       # Dependencias de Python (para la imagen)
â”œâ”€â”€ Dockerfile             # Imagen de la app
â”œâ”€â”€ docker-compose.yml     # OrquestaciÃ³n (app + ollama)
â””â”€â”€ README.md              # Este archivo
</code></pre>

<h2>ğŸ—ï¸ Arquitectura del Sistema</h2>

<h3>Diagrama de Arquitectura</h3>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USUARIO                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NAVEGADOR WEB                                â”‚
â”‚                    (localhost:8501)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ HTTP/WebSocket
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONTAINER: APP                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                 STREAMLIT APP                           â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚   INTERFAZ      â”‚  â”‚      PROCESAMIENTO          â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   DE USUARIO    â”‚  â”‚         DE PDFs             â”‚  â”‚   â”‚
â”‚  â”‚  â”‚                 â”‚  â”‚                             â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Upload PDF    â”‚  â”‚ â€¢ ExtracciÃ³n de texto       â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Chat UI       â”‚  â”‚ â€¢ Chunking inteligente      â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Historial     â”‚  â”‚ â€¢ GeneraciÃ³n embeddings     â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚              LANGCHAIN ORCHESTRATOR                 â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  â”‚   RAG       â”‚  â”‚   CHAIN     â”‚  â”‚   OUTPUT    â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  ENGINE     â”‚  â”‚  MANAGER    â”‚  â”‚  PROCESSOR  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONTAINER: OLLAMA                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                 LLM SERVICE                             â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚              LLAMA3:3B MODEL                        â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Procesamiento de consultas                       â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ GeneraciÃ³n de respuestas                         â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Contexto de conversaciÃ³n                         â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VECTOR STORE                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    CHROMADB                             â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚   EMBEDDINGS    â”‚  â”‚      METADATA               â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   STORAGE       â”‚  â”‚      STORAGE                 â”‚  â”‚   â”‚
â”‚  â”‚  â”‚                 â”‚  â”‚                             â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Vector chunks â”‚  â”‚ â€¢ InformaciÃ³n del PDF        â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Similarity    â”‚  â”‚ â€¢ Timestamps                 â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   search        â”‚  â”‚ â€¢ Chunk indices              â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FLUJO DE DATOS â”‚
â”‚ â”‚
â”‚ 1. Usuario sube PDF â†’ Streamlit â”‚
â”‚ 2. Streamlit â†’ Procesamiento â†’ Chunks + Embeddings â”‚
â”‚ 3. Embeddings â†’ ChromaDB (Vector Store) â”‚
â”‚ 4. Usuario hace pregunta â†’ Streamlit â”‚
â”‚ 5. Streamlit â†’ RAG Engine â†’ ChromaDB (bÃºsqueda semÃ¡ntica) â”‚
â”‚ 6. RAG Engine â†’ Ollama (LLM) con contexto â”‚
â”‚ 7. Ollama â†’ Respuesta â†’ Streamlit â†’ Usuario â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>

<h3>Componentes Principales</h3>
<ul>
  <li><strong>Frontend (Streamlit)</strong>: Interfaz de usuario para upload y chat</li>
  <li><strong>Backend (Python)</strong>: LÃ³gica de procesamiento y orquestaciÃ³n</li>
  <li><strong>LLM Service (Ollama)</strong>: Modelo de lenguaje local para generaciÃ³n de respuestas</li>
  <li><strong>Vector Database (ChromaDB)</strong>: Almacenamiento de embeddings para bÃºsqueda semÃ¡ntica</li>
  <li><strong>RAG Engine (LangChain)</strong>: OrquestaciÃ³n del flujo de Retrieval-Augmented Generation</li>
</ul>

<h2>â“Preguntas frecuentes</h2>
<ul>
  <li><strong>Â¿Por quÃ© tarda tanto la primera vez?</strong><br>
    Porque Docker construye la imagen (instala dependencias) y Ollama descarga el modelo <code>llama3:3b</code>. DespuÃ©s de esa primera preparaciÃ³n, los arranques siguientes usan la cachÃ© y son mucho mÃ¡s rÃ¡pidos.</li>
  <li><strong>Â¿Necesito Python/venv en mi mÃ¡quina?</strong><br>
    No. Todo corre dentro de los contenedores.</li>
  <li><strong>Â¿CÃ³mo actualizo el cÃ³digo?</strong><br>
    Cambia tus archivos y luego:
    <pre><code>docker compose build app
docker compose up -d
</code></pre>
  </li>
  <li><strong>Â¿Por quÃ© la app se demora tanto en enviar mensajes? </strong> <br> Es un problema comÃºn que notÃ© al momento de dockerizar la aplicaciÃ³n. Puede ser que el modelo de lenguaje es muy lento o se demora haciendo la solicitud al llm.</li>
</ul>

<h2>ğŸ“š TecnologÃ­as</h2>
<ul>
  <li><strong>Streamlit</strong> â€” interfaz web</li>
  <li><strong>LangChain</strong> â€” orquestaciÃ³n de LLM + RAG</li>
  <li><strong>ChromaDB</strong> â€” vector store</li>
  <li><strong>Sentence Transformers</strong> â€” embeddings</li>
  <li><strong>Ollama</strong> â€” LLM local (modelo <code>llama3:3b</code>)</li>
  <li><strong>Docker &amp; Docker Compose</strong> â€” despliegue y orquestaciÃ³n</li>
</ul>

<h3>ğŸ¯ JustificaciÃ³n de la Stack TecnolÃ³gica</h3>
<ul>
  <li><strong>Streamlit</strong> â€” Front-end ideal para prototipos de IA, desarrollo "rÃ¡pido" y recomendado para proyectos con IA.</li>
  <li><strong>LangChain</strong> â€” Framework estÃ¡ndar para RAG con integraciÃ³n con Ollama</li>
  <li><strong>ChromaDB</strong> â€” Vector store ligero y eficiente para aplicaciones locales</li>
  <li><strong>Sentence Transformers</strong> â€” Embeddings de alta calidad en espaÃ±ol/inglÃ©s, optimizados para RAG</li>
  <li><strong>Ollama</strong> â€” LLM local sin dependencias externas, escogÃ­ el modelo llama3:3b por el balance calidad/velocidad</li>
  <li><strong>Docker</strong> â€” Para reproducir el entorno en otras mÃ¡quinas.</li>
</ul>

<h2>âš ï¸ Limitaciones Actuales</h2>
<ul>
  <li><strong>Interfaz Visual</strong>: La interfaz actual es funcional pero bÃ¡sica, necesita mejoras en diseÃ±o y UX para verse mÃ¡s profesional e intuitivo de usar.</li>
  <li><strong>Rendimiento</strong>: El procesamiento de PDFs y las respuestas del LLM pueden ser lentas, especialmente con archivos grandes.</li>
  <li><strong>Persistencia</strong>: No hay sistema de usuarios ni memoria de archivos procesados anteriormente</li>
  <li><strong>Seguridad</strong>: Falta implementar autenticaciÃ³n, autorizaciÃ³n y validaciÃ³n de archivos</li>
  <li><strong>Escalabilidad</strong>: Limitado a un solo usuario por sesiÃ³n.</li>
  <li><strong>Modelo LLM</strong>: El modelo llama3:3b es rÃ¡pido (lo probÃ© con el modelo estandar) pero puede tener limitaciones en calidad de respuestas complejas</li>
</ul>

<h2>ğŸš€ Roadmap de Mejoras Futuras</h2>

<h3>ğŸ¨ Fase 1: Mejoras de Interfaz y UX</h3>
<ul>
  <li>RediseÃ±o completo de la interfaz con componentes modernos y responsive</li>
  <li>ImplementaciÃ³n de temas claro/oscuro</li>
  <li>Mejoras en la visualizaciÃ³n de PDFs (zoom, navegaciÃ³n por pÃ¡ginas)</li>
  <li>Indicadores de progreso y feedback visual mejorado</li>
  <li>Historial de conversaciones en la misma sesiÃ³n</li>
</ul>

<h3>âš¡ Fase 2: OptimizaciÃ³n de Rendimiento</h3>
<ul>
  <li>Probar otros modelos para evaluar rendimiento</li>
  <li>ImplementaciÃ³n de procesamiento asÃ­ncrono de PDFs</li>
  <li>OptimizaciÃ³n del chunking y embeddings para mayor velocidad</li>
  <li>Cache de embeddings para archivos procesados previamente</li>
  <li>Procesamiento en lotes para mÃºltiples archivos</li>
  <li>EvaluaciÃ³n de modelos LLM mÃ¡s rÃ¡pidos manteniendo calidad</li>
  <li>ImplementaciÃ³n de framework de testing robusto (pytest, unittest) con tests unitarios, de integraciÃ³n y de rendimiento</li>
</ul>

<h3>ğŸ‘¥ Fase 3: Sistema de Usuarios y Persistencia</h3>
<ul>
  <li>Sistema de autenticaciÃ³n y registro de usuarios</li>
  <li>Base de datos para almacenar historial de archivos y conversaciones</li>
  <li>Dashboard personalizado con estadÃ­sticas de uso</li>
  <li>Compartir archivos y conversaciones entre usuarios</li>
  <li>Sistema de favoritos y etiquetas para organizar contenido</li>
</ul>

<h3>ğŸ”’ Fase 4: Seguridad y Robustez</h3>
<ul>
  <li>ValidaciÃ³n y sanitizaciÃ³n de archivos PDF</li>
  <li>Rate limiting para prevenir abuso</li>
  <li>EncriptaciÃ³n de datos sensibles</li>
  <li>AuditorÃ­a de acceso y logs de seguridad</li>
</ul>

<h3>ğŸŒ Fase 5: Funcionalidades Avanzadas</h3>
<ul>
  <li>SeparaciÃ³n de backend y frontend para mejor escalabilidad y mantenimiento. QuizÃ¡s usar FastAPI ya que se integra bien con proyectos de python.</li>
  <li>API REST para integraciÃ³n con otros sistemas</li>
  <li>Webhooks para notificaciones</li>
  <li>IntegraciÃ³n con sistemas de almacenamiento en la nube</li>
  <li>AnÃ¡lisis avanzado de documentos (extracciÃ³n de tablas, imÃ¡genes)</li>
  <li>Sistema de plugins para funcionalidades personalizables</li>
</ul>

<h3>ğŸ“Š Fase 6: Monitoreo y Analytics</h3>
<ul>
  <li>Dashboard de mÃ©tricas de rendimiento</li>
  <li>AnÃ¡lisis de uso y patrones de consulta</li>
  <li>Sistema de alertas para problemas de rendimiento</li>
  <li>Reportes automÃ¡ticos de calidad de respuestas</li>
  <li>IntegraciÃ³n con herramientas de observabilidad</li>
</ul>

<h2>ğŸ¤ Contribuciones</h2>
<p>Â¡Las contribuciones son bienvenidas! Puedes hacer Fork y aportar, Este es un prototipo por lo que puede que continÃºe este proyecto para un portafolio mÃ¡s robusto. Gracias por revisar mi proyecto :)</p>
